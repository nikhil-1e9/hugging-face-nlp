{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhil-1e9/hugging-face-nlp/blob/main/chapter1/section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemNup9WIZjj"
      },
      "source": [
        "# Transformers, what can they do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLIReQBIZjm"
      },
      "source": [
        "## Install the Transformers, Datasets, and Evaluate libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yV4CjDEQIZjm"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "lJi4kx2mhZ4j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L9hhgWXEIZjn",
        "outputId": "47230aab-f331-49ae-a359-f4b8557c2f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9986742734909058}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"There should be peace and harmony on Earth.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fglS1iBIIZjo",
        "outputId": "c5db0972-c5ae-4aa1-bcaf-79535427aafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998372793197632},\n",
              " {'label': 'NEGATIVE', 'score': 0.9987448453903198}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"That was the best day of my life.\", \"He is suffering from depression\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Shot Classification"
      ],
      "metadata": {
        "id": "ZFsjng48hhXs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iC2OOdVRIZjo",
        "outputId": "31381ba5-9f57-44df-aaee-60085d7260e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'Hurry, there is a storm coming!',\n",
              " 'labels': ['weather',\n",
              "  'business',\n",
              "  'geography',\n",
              "  'technology',\n",
              "  'politics',\n",
              "  'education',\n",
              "  'art'],\n",
              " 'scores': [0.9494072794914246,\n",
              "  0.02139625884592533,\n",
              "  0.00732309278100729,\n",
              "  0.007233645301312208,\n",
              "  0.005403521936386824,\n",
              "  0.004826264455914497,\n",
              "  0.004409927874803543]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"Hurry, there is a storm coming!\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\", \"geography\", \"weather\", \"art\", \"technology\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If no model is supplied it automatically selects the default model. There are different default models for different tasks. For example - **GPT-2** is the default model for **text generation**."
      ],
      "metadata": {
        "id": "6hPw40OPkGgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation"
      ],
      "metadata": {
        "id": "J6rwGnp6iL-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qmVN8KBKIZjo",
        "outputId": "cdf569f9-761d-4112-bcf3-f9aa9d3c81e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'ChatGPT has revolutionized the way we spend our money. With a focus on simplicity, high performance, and speed, our services have become popular worldwide. Today, almost 100 million users use us daily, but our technology is getting better daily thanks'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"ChatGPT has revolutionized the\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It just completes the sentence which has no relevance to ChatGPT because it does not know what it is."
      ],
      "metadata": {
        "id": "Edc4i_0vikbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model to be used can be provided in the pipeline itself. Moreover, we can also specify the output length and number of different results by providing values for `max_length` and `num_return_sequences`"
      ],
      "metadata": {
        "id": "qVRR6xNbi8fC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f9DtEW-2IZjp",
        "outputId": "1934c609-93e9-4180-a388-5ef0ca02b7f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Just writing some random sentence for today's series. It does not just make it funny, it's funny. It also gives the series a fun and\"},\n",
              " {'generated_text': 'Just writing some random sentence that we will always be happy to show them are true, yet the idea is simple and elegant.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Text generation using DistilGPT-2 model\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"Just writing some random sentence\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling masked values"
      ],
      "metadata": {
        "id": "QCKkboEhkn56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hi3cEykOIZjp",
        "outputId": "d3779801-6e0d-4f85-d6f7-74a2894ede50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.38455602526664734,\n",
              "  'token': 4044,\n",
              "  'token_str': ' gap',\n",
              "  'sequence': 'There is a huge gap between the rich and the poor.'},\n",
              " {'score': 0.16631922125816345,\n",
              "  'token': 35957,\n",
              "  'token_str': ' gulf',\n",
              "  'sequence': 'There is a huge gulf between the rich and the poor.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"There is a huge <mask> between the rich and the poor.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "QAWX4Z6jlF76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q6FI7_CYIZjp",
        "outputId": "9bec9857-ebfa-41d8-968c-57cdec3eacbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.99814796,\n",
              "  'word': 'Jon Snow',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.97871894,\n",
              "  'word': 'Game of Thrones',\n",
              "  'start': 34,\n",
              "  'end': 49},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.998781,\n",
              "  'word': 'London',\n",
              "  'start': 53,\n",
              "  'end': 59}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Jon Snow and I work at Game of Thrones in London.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "YZb3Jrz8mCep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-GkfjSh3IZjq",
        "outputId": "a382d8d9-767d-4566-c9ee-a0ad42a57303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9564961194992065, 'start': 39, 'end': 42, 'answer': 'red'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"What is the color of the bag?\",\n",
        "    context=\"I am standing on 46th crossroad with a red color bag in my hand.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Summarization"
      ],
      "metadata": {
        "id": "NdwkjZ9imsKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GMUFzG5SIZjq",
        "outputId": "a7759137-e980-4e0b-db3a-d11f33d3a3e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' Philosophy of Education is a label applied to the study of the purpose, process, nature, and ideals of education . Many educationalists consider it a weak and woolly field, too far removed from the practical applications of the real world to be useful . But philosophers dating back to Plato and the Ancient Greeks have given the area much thought and emphasis .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    Philosophy of Education is a label applied to the study of the purpose, process, nature, and ideals of education.\n",
        "    It can be considered a branch of both philosophy and education. Education can be defined as the teaching and\n",
        "    learning of specific skills, and the imparting of knowledge, judgment, and wisdom, and is something broader than\n",
        "    the societal institution of education we often speak of.\n",
        "\n",
        "    Many educationalists consider it a weak and woolly field, too far removed from the practical applications of the\n",
        "    real world to be useful. But philosophers dating back to Plato and the Ancient Greeks have given the area much\n",
        "    thought and emphasis, and there is little doubt that their work has helped shape the practice of education over\n",
        "    the millennia.\n",
        "\n",
        "    Plato is the earliest important educational thinker, and education is an essential element in “The Republic” (his\n",
        "    most important work on philosophy and political theory, written around 360 B.C.). In it, he advocates some rather\n",
        "    extreme methods: removing children from their mothers’ care and raising them as wards of the state, and\n",
        "    differentiating children suitable to the various castes, the highest receiving the most education, so that they\n",
        "    could act as guardians of the city and care for the less able. He believed that education should be holistic,\n",
        "    including facts, skills, physical discipline, music, and art. Plato believed that talent and intelligence are not\n",
        "    distributed genetically and thus are found in children born to all classes, although his proposed system of\n",
        "    selective public education for an educated minority of the population does not follow a democratic model.\n",
        "\n",
        "    Aristotle considered human nature, habit, and reason to be equally important forces to be cultivated in education,\n",
        "    the ultimate aim of which should be to produce good and virtuous citizens. He proposed that teachers lead their\n",
        "    students systematically, and that repetition be used as a key tool to develop good habits, unlike Socrates’ emphasis\n",
        "    on questioning his listeners to bring out their ideas. He emphasized the balancing of the theoretical and practical\n",
        "    aspects of subjects taught, among which he explicitly mentions reading, writing, mathematics, music, physical\n",
        "    education, literature, history, and a wide range of sciences, as well as play, which he also considered important.\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language translation"
      ],
      "metadata": {
        "id": "_tc60eVanq8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_8oErTmtIZjq",
        "outputId": "bb7c0145-03f9-4908-de90-127b911244d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'This is your boarding pass. The boarding will take place at Gate 3'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Voici votre carte d'embarquement. L'embarquement aura lieu à la porte 3\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}